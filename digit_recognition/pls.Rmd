---
title: "Exercise - PLS2. ZIP data"
output: html_document
---

```{r}
library(pls)
```

**1**

**Read the "zip_train.dat" and "zip_test.dat" files provided.**

```{r}
train <- read.table('data/zip_train.dat')
test <- read.table('data/zip_test.dat')
```

**Select the same 5% random sample (without replacement) of the train data used in exercise 1.**

Our training dataset consists of 7291 samples. By choosing 5\% of data we shrink it down to 364. By doing so we will try to fit the model with number of observations close to the number	 of the parameters (in this case 256). Such a datasets are the target of PCR method.

```{r}
train.samples <- sample(nrow(train), 0.05 * nrow(train))
n <- length(train.samples)
```

**Use this sample as your training data, and the complete test data for testing.**

```{r}
X.train <- as.matrix(train[train.samples, -1])
Y.train <- train[train.samples, 1]

X.test <- as.matrix(test[, -1])
Y.test <- test[, 1]
```

**2**

**Define the response matrix (Y) and the predictor matrix (X).**

X matrix was ready before.

Because the digits have to be treated as categorical variables they will be represented by one-hot encoding.

```{r}
oneHot <- function(x) {
  result <- rep(0, 10)
  result[x + 1] <- 1
  return(result)
}

Y.train.one.hot <- t(sapply(Y.train, oneHot))
Y.test.one.hot <- t(sapply(Y.test, oneHot))
```

**Center the predictor matrix.**

```{r}
X.train <- scale(X.train, scale = FALSE)
X.test <- scale(X.test, scale = FALSE)
```

**3**

**Perform a PLSR2 using “CV” or “LOO” for validation.**

```{r}
p2 <- plsr(Y.train.one.hot ~ X.train, ncomp = 50, validation = "LOO")
```

**Decide how many components you retain for prediction?**

In order to assess the models the bias-corrected cross-validation estimate will be plotted as the function of the number of components.

Each response the `adjCV` score will be weighted by the number of it's representatives.


```{r}
adjCV <- RMSEP(p2)$val[2,,]
adjCV.weighted <- as.numeric(colSums(Y.train.one.hot) %*% adjCV / sum(colSums(Y.train.one.hot)))
plot(adjCV.weighted)
```

The number of components that optimize the value of `adjCV` is:

```{r}
c <- which.min(adjCV.weighted) - 1
c
```

**4**

**Predict the responses in the test data, be aware of the appropriate centering.**

```{r}
m <- lm(Y.train.one.hot ~ p2$scores[, 1:c])

projection <- X.test %*% p2$projection[, 1:c]
pred.one.hot <- projection %*% m$coefficients[-1, ] + m$coefficients[1, ]
```

**Compute the average R2 in the test data.**

```{r}
getR2 <- function(Y.one.hot, pred.one.hot) {
  TSS = diag(t(Y.one.hot) %*% Y.test.one.hot)
  RSS = diag(t(Y.one.hot - pred.one.hot) %*% (Y.one.hot - pred.one.hot))
  return(1 - (RSS / TSS))
}

r2 <- getR2(Y.test.one.hot, pred.one.hot)
sum(colSums(Y.train.one.hot) * r2) / sum(colSums(Y.train.one.hot))
```

**5**

**Assign every test individual to the maximum response.**

```{r}
pred <- apply(pred.one.hot, 1, which.max) - 1
```

**Compute the error rate.**

```{r}
sum(Y.test != pred) / length(pred)
```

